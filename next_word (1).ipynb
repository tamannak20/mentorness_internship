{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d82bbfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import numpy as np                                                   #for numerical operations\n",
    "import pandas as pd                                                  #for manipulation\n",
    "import matplotlib.pyplot as plt                                      #for creating interactive visualizations\n",
    "import os\n",
    "import pickle                                                        #used for saving/loading trained machine learning models\n",
    "import tensorflow as tf                                              #for building/training deep learning models\n",
    "from tensorflow import keras                                         #provide interface for building/training neural networks\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer            #to convert text into a sequence of tokens or words \n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense           #Embedding->word embeddings,\n",
    "                                                                     #LSTM ->type of RNN layer,Dense->fully connected layer\n",
    "from tensorflow.keras.models import Sequential         #linear stack of layers in Keras(allow us to build model layer by layer)\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  #ensure that all sequences in a list have the same length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4b2696b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the corpus is: : 610921\n"
     ]
    }
   ],
   "source": [
    "#read the data file\n",
    "path=r\"C:\\Users\\taman\\Downloads\\Sherlock Holmes Dataset.txt\"         #path of your text file\n",
    "text = open(path).read().lower()                                     #read and convert it into lowercase\n",
    "print('length of the corpus is: :', len(text))                       #checking length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c0e8719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocessing\n",
    "#-----Tokenization------process of breaking down a text into smaller units called tokens\n",
    "#Create a tokenizer\n",
    "tokenizer = Tokenizer()                   \n",
    "#Fit the tokenizer on the text data\n",
    "tokenizer.fit_on_texts([text])   #pass text as input then analyze text,builds a vocabulary of unique words/assigns numerical index to each\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "total_words                      #total number of distinct words in the text     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69f5af7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "#Loop through each line in the text\n",
    "for line in text.split('\\n'):                              #assuming 'text' is a multiline string then split text into lines\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]   #Tokenize the current line using the tokenizer\n",
    "    # Create n-gram sequences from the tokenized line\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ce09e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates the maximum length among all the sequences \n",
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "\n",
    "#pad_sequences-->ensure all sequences in input_sequences have same length,\n",
    "#max_sequence->maximum length of the sequences after padding,\n",
    "#'pre'->padding should be added to the beginning of each sequence\n",
    "#np.array->convert list of sequences into numpy array\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e496467",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input sequences are split into two arrays, ‘X’ and ‘y’\n",
    "X = input_sequences[:, :-1]  #except for the last column\n",
    "y = input_sequences[:, -1]   #values of the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72f302a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,    0,    0,    1],\n",
       "       [   0,    0,    0, ...,    0,    1, 1561],\n",
       "       [   0,    0,    0, ...,    1, 1561,    5],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,   28,    1, 8198],\n",
       "       [   0,    0,    0, ...,    1, 8198, 8199],\n",
       "       [   0,    0,    0, ..., 8198, 8199, 3187]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "430e7b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1561,    5,  129, ..., 8199, 3187, 3186])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d9ee9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming a list of class labels y into a NumPy array\n",
    "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f3d102c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 17, 100)           820000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               117248    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8200)              1057800   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,995,048\n",
      "Trainable params: 1,995,048\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Adding an Embedding layer\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "\n",
    "# Adding an LSTM layer\n",
    "model.add(LSTM(128))\n",
    "\n",
    "# Adding a Dense layer\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "# Printing the model summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdce75ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3010/3010 [==============================] - 66s 22ms/step - loss: 6.2600 - accuracy: 0.0745\n",
      "Epoch 2/50\n",
      "3010/3010 [==============================] - 76s 25ms/step - loss: 5.5412 - accuracy: 0.1227\n",
      "Epoch 3/50\n",
      "3010/3010 [==============================] - 70s 23ms/step - loss: 5.1648 - accuracy: 0.1463\n",
      "Epoch 4/50\n",
      "3010/3010 [==============================] - 70s 23ms/step - loss: 4.8472 - accuracy: 0.1624\n",
      "Epoch 5/50\n",
      "3010/3010 [==============================] - 69s 23ms/step - loss: 4.5656 - accuracy: 0.1785\n",
      "Epoch 6/50\n",
      "3010/3010 [==============================] - 69s 23ms/step - loss: 4.3043 - accuracy: 0.1966\n",
      "Epoch 7/50\n",
      "3010/3010 [==============================] - 70s 23ms/step - loss: 4.0549 - accuracy: 0.2177\n",
      "Epoch 8/50\n",
      "3010/3010 [==============================] - 70s 23ms/step - loss: 3.8157 - accuracy: 0.2431\n",
      "Epoch 9/50\n",
      "3010/3010 [==============================] - 70s 23ms/step - loss: 3.5888 - accuracy: 0.2719\n",
      "Epoch 10/50\n",
      "3010/3010 [==============================] - 69s 23ms/step - loss: 3.3740 - accuracy: 0.3006\n",
      "Epoch 11/50\n",
      "3010/3010 [==============================] - 70s 23ms/step - loss: 3.1748 - accuracy: 0.3326\n",
      "Epoch 12/50\n",
      "3010/3010 [==============================] - 70s 23ms/step - loss: 2.9885 - accuracy: 0.3661\n",
      "Epoch 13/50\n",
      "3010/3010 [==============================] - 72s 24ms/step - loss: 2.8187 - accuracy: 0.3939\n",
      "Epoch 14/50\n",
      "3010/3010 [==============================] - 77s 26ms/step - loss: 2.6611 - accuracy: 0.4228\n",
      "Epoch 15/50\n",
      "3010/3010 [==============================] - 73s 24ms/step - loss: 2.5133 - accuracy: 0.4536\n",
      "Epoch 16/50\n",
      "3010/3010 [==============================] - 66s 22ms/step - loss: 2.3795 - accuracy: 0.4795\n",
      "Epoch 17/50\n",
      "3010/3010 [==============================] - 68s 22ms/step - loss: 2.2546 - accuracy: 0.5043\n",
      "Epoch 18/50\n",
      "3010/3010 [==============================] - 65s 21ms/step - loss: 2.1417 - accuracy: 0.5265\n",
      "Epoch 19/50\n",
      "3010/3010 [==============================] - 64s 21ms/step - loss: 2.0343 - accuracy: 0.5496\n",
      "Epoch 20/50\n",
      "3010/3010 [==============================] - 64s 21ms/step - loss: 1.9368 - accuracy: 0.5708\n",
      "Epoch 21/50\n",
      "3010/3010 [==============================] - 64s 21ms/step - loss: 1.8461 - accuracy: 0.5884\n",
      "Epoch 22/50\n",
      "3010/3010 [==============================] - 64s 21ms/step - loss: 1.7638 - accuracy: 0.6035\n",
      "Epoch 23/50\n",
      "3010/3010 [==============================] - 64s 21ms/step - loss: 1.6856 - accuracy: 0.6224\n",
      "Epoch 24/50\n",
      "3010/3010 [==============================] - 64s 21ms/step - loss: 1.6133 - accuracy: 0.6378\n",
      "Epoch 25/50\n",
      "3010/3010 [==============================] - 64s 21ms/step - loss: 1.5465 - accuracy: 0.6520\n",
      "Epoch 26/50\n",
      "3010/3010 [==============================] - 64s 21ms/step - loss: 1.4852 - accuracy: 0.6650\n",
      "Epoch 27/50\n",
      "3010/3010 [==============================] - 64s 21ms/step - loss: 1.4288 - accuracy: 0.6773\n",
      "Epoch 28/50\n",
      "3010/3010 [==============================] - 64s 21ms/step - loss: 1.3757 - accuracy: 0.6885\n",
      "Epoch 29/50\n",
      "3010/3010 [==============================] - 66s 22ms/step - loss: 1.3269 - accuracy: 0.7001\n",
      "Epoch 30/50\n",
      "3010/3010 [==============================] - 66s 22ms/step - loss: 1.2798 - accuracy: 0.7092\n",
      "Epoch 31/50\n",
      "3010/3010 [==============================] - 64s 21ms/step - loss: 1.2368 - accuracy: 0.7198\n",
      "Epoch 32/50\n",
      "3010/3010 [==============================] - 64s 21ms/step - loss: 1.1981 - accuracy: 0.7270\n",
      "Epoch 33/50\n",
      "3010/3010 [==============================] - 65s 22ms/step - loss: 1.1598 - accuracy: 0.7355\n",
      "Epoch 34/50\n",
      "3010/3010 [==============================] - 66s 22ms/step - loss: 1.1228 - accuracy: 0.7439\n",
      "Epoch 35/50\n",
      "3010/3010 [==============================] - 67s 22ms/step - loss: 1.0922 - accuracy: 0.7497\n",
      "Epoch 36/50\n",
      "3010/3010 [==============================] - 66s 22ms/step - loss: 1.0614 - accuracy: 0.7565\n",
      "Epoch 37/50\n",
      "3010/3010 [==============================] - 65s 22ms/step - loss: 1.0329 - accuracy: 0.7625\n",
      "Epoch 38/50\n",
      "3010/3010 [==============================] - 64s 21ms/step - loss: 1.0050 - accuracy: 0.7684\n",
      "Epoch 39/50\n",
      "3010/3010 [==============================] - 65s 22ms/step - loss: 0.9803 - accuracy: 0.7737\n",
      "Epoch 40/50\n",
      "3010/3010 [==============================] - 68s 23ms/step - loss: 0.9539 - accuracy: 0.7795\n",
      "Epoch 41/50\n",
      "3010/3010 [==============================] - 66s 22ms/step - loss: 0.9333 - accuracy: 0.7827\n",
      "Epoch 42/50\n",
      "3010/3010 [==============================] - 66s 22ms/step - loss: 0.9114 - accuracy: 0.7893\n",
      "Epoch 43/50\n",
      "3010/3010 [==============================] - 66s 22ms/step - loss: 0.8923 - accuracy: 0.7919\n",
      "Epoch 44/50\n",
      "3010/3010 [==============================] - 66s 22ms/step - loss: 0.8731 - accuracy: 0.7967\n",
      "Epoch 45/50\n",
      "3010/3010 [==============================] - 65s 21ms/step - loss: 0.8550 - accuracy: 0.8006\n",
      "Epoch 46/50\n",
      "3010/3010 [==============================] - 64s 21ms/step - loss: 0.8365 - accuracy: 0.8049\n",
      "Epoch 47/50\n",
      "3010/3010 [==============================] - 64s 21ms/step - loss: 0.8233 - accuracy: 0.8076\n",
      "Epoch 48/50\n",
      "3010/3010 [==============================] - 64s 21ms/step - loss: 0.8091 - accuracy: 0.8106\n",
      "Epoch 49/50\n",
      "3010/3010 [==============================] - 65s 22ms/step - loss: 0.7935 - accuracy: 0.8131\n",
      "Epoch 50/50\n",
      "3010/3010 [==============================] - 66s 22ms/step - loss: 0.7813 - accuracy: 0.8168\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  \n",
    "#verbose->Controls amount of information printed during training i.e =1 (progress bars/information displayed for each epoch.)\n",
    "#epochs->number of times the model will iterate over the entire training dataset\n",
    "lstm=model.fit(X, y,epochs=50, verbose=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5be2b8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.260003089904785,\n",
       " 5.541238307952881,\n",
       " 5.1647539138793945,\n",
       " 4.847181797027588,\n",
       " 4.5656328201293945,\n",
       " 4.304337024688721,\n",
       " 4.054908752441406,\n",
       " 3.8156862258911133,\n",
       " 3.588751792907715,\n",
       " 3.3739705085754395,\n",
       " 3.17478346824646,\n",
       " 2.9884605407714844,\n",
       " 2.818697929382324,\n",
       " 2.661052703857422,\n",
       " 2.513260841369629,\n",
       " 2.379453420639038,\n",
       " 2.2546467781066895,\n",
       " 2.1416709423065186,\n",
       " 2.03425931930542,\n",
       " 1.9367928504943848,\n",
       " 1.8461313247680664,\n",
       " 1.7637544870376587,\n",
       " 1.6855756044387817,\n",
       " 1.613268494606018,\n",
       " 1.5465370416641235,\n",
       " 1.4851809740066528,\n",
       " 1.4287761449813843,\n",
       " 1.3756815195083618,\n",
       " 1.3268758058547974,\n",
       " 1.2798045873641968,\n",
       " 1.2367539405822754,\n",
       " 1.1981438398361206,\n",
       " 1.159767746925354,\n",
       " 1.1228121519088745,\n",
       " 1.0922101736068726,\n",
       " 1.061352252960205,\n",
       " 1.0328521728515625,\n",
       " 1.0049593448638916,\n",
       " 0.9803075790405273,\n",
       " 0.9538580179214478,\n",
       " 0.9332873225212097,\n",
       " 0.9114183783531189,\n",
       " 0.892274796962738,\n",
       " 0.8730690479278564,\n",
       " 0.8549835085868835,\n",
       " 0.8364862203598022,\n",
       " 0.8233078122138977,\n",
       " 0.8090895414352417,\n",
       " 0.793489933013916,\n",
       " 0.7812923192977905]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lstm.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a257cadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "i found in the direction of the wood sherlock\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"i found in\"\n",
    "next_words = 6\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8e81bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
